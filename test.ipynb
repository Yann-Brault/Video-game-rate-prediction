{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Traitement Automatique de texte en IA projet\n",
    "> \n",
    "> > Antoine Vidal-Mazuy - Yann Brault\n",
    "> > 28/12/2021 Université Côtes d'azur\n",
    "\n",
    "<br>\n",
    "\n",
    "# Table of content\n",
    "[Pre Processing](#pre-processing)\n",
    "\n",
    "> ## Introduction\n",
    "\n",
    "<br>\n",
    "\n",
    "> blablzqdbd bzqjkdb qz\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> ## Datasets utilisés\n",
    ">\n",
    "> > #### Datasets de base:\n",
    "\n",
    "<br>\n",
    "\n",
    "> Nous avons décidé de construire nous même notre base de donnée. Pour cela nous avons conçu un petit scraper su site Jeux-Video.com. <br>\n",
    "> Le code source du scraper se trouve dans le lien suivant [JVCScraper](https://github.com/Brotherta/JVCScraper).\n",
    "\n",
    "<br>\n",
    "\n",
    "> Nous avons récupéré tous les commentaires et notes d'utilisateurs sur environ 50 jeux, pour un total de 102000 avis. Puis nous les avons sauvegarder sous forme de fichiers csv. <br>\n",
    "> Nous avons dû nettoyer les avis de tous les charactères spéciaux, des mots de liaisons, les urls et tout ce qui n'apportait rien à la compréhension de l'avis.\n",
    "> Afin de nettoyer plus facilement les commentaires nous avons créer une classe **CleanData** qui nous permettait d'effectuer plusieurs tâches de pré-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "\n",
    "class CleanData:\n",
    "\n",
    "    def __init__(self, max_words, df: DataFrame = None) -> None:\n",
    "        self.max_words = max_words\n",
    "        self.df = df\n",
    "        self.unused_chars = ',|;|\\&|\\#|\\@|\\%|\\:|\\>|\\<|\\(|\\)|\\{|\\}|\\=|\\+|\\_|\\[|\\}|\\^|\\*|\\!|\\?|\\/|\\¨|\\~|\\\\\\|\\§|\\||[0-9]|\\[|\\]|\\\"'\n",
    "        self.connecting_words = [\n",
    "            \"c'est\", \"ces\", \"ses\", \"s'est\", \"a\", \"de\", \"du\", \n",
    "            \"et\", \"le\", \"les\", \"un\", \"une\", \"pour\", \"sur\", \"etc\", \"est\", \"c\",\n",
    "            'la', \"jeu\", \"que\", \"des\", \"en\", \"ce\", \"qu\", \"ca\", \"y\", \"je\", \"sa\", \"son\",\n",
    "            \"au\", \"ai\", \"mon\", \"ma\", \"mes\", \"qui\", \"je\", \"tu\", \"il\", \"ils\", \"elles\", \"elle\", \"vous\", \"nous\",\n",
    "            \"qu'il\", \"qu'elle\", \"qu'ils\", \"qu'elles\", \"qu'on\",\n",
    "            \"on\", \"se\", \"par\"]\n",
    "        self.urls = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b'\n",
    "            \n",
    "        self.spell = SpellChecker(language='fr')\n",
    "\n",
    "    def correction_spelling(self, review):\n",
    "        \"\"\" \n",
    "        try to elimiminates the unknown words of a sentence, and \n",
    "        replacing it by a correct word. \n",
    "        \"\"\"\n",
    "        \n",
    "        review_list = review.split(\" \")\n",
    "        \n",
    "        bad = []\n",
    "        for word in review_list:\n",
    "            if word != \" \":\n",
    "                bad = self.spell.unknown(review_list)\n",
    "\n",
    "        new_list = []\n",
    "        for word in review_list:\n",
    "            if word in bad:\n",
    "                new_list.append(self.spell.correction(word))\n",
    "            else:\n",
    "                new_list.append(word)\n",
    "        \n",
    "        return ' '.join(new_list)\n",
    "\n",
    "    def replace_nan(self):\n",
    "        \"\"\"\n",
    "        Replace nan by 'bon' or 'mauvais' in the dataframe.\n",
    "        \"\"\"\n",
    "        to_drop = []\n",
    "        for i in self.df.index:\n",
    "            r = self.df['avis'][i]\n",
    "            if pd.isna(r) or r in ['nan', 'Nan'] or type(r) == float:\n",
    "                to_drop.append(i)\n",
    "        print(\"droped nan : \", len(to_drop))\n",
    "        self.df = self.df.drop(to_drop)\n",
    "\n",
    "    def remove_urls(self, review):\n",
    "        review = re.sub(self.urls, '', review, flags=re.MULTILINE)\n",
    "        return(review)\n",
    "\n",
    "    def clean_str(self, review):\n",
    "        \"\"\"\n",
    "        Remove special characters from the string.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(review) > 0 or review != None:\n",
    "            review = re.sub(self.unused_chars, ' ', review)\n",
    "            review = review.replace('.', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('\\n', ' ')\n",
    "            review = review.lower()\n",
    "            review = re.sub(' +', ' ', review)\n",
    "            review = re.sub(r' (?! ) ', '', review) #removing single characters\n",
    "       \n",
    "        return review\n",
    "    \n",
    "    def clean_stop_words(self, review):\n",
    "        review_list = review.split(\" \")\n",
    "\n",
    "        new_list = []\n",
    "        for word in review_list:\n",
    "            if word != \" \" and not word in self.connecting_words and len(word) > 1:\n",
    "                new_list.append(word)\n",
    "\n",
    "        return ' '.join(new_list)\n",
    "\n",
    "    def clean_review(self, review):\n",
    "        review = self.clean_str(review)\n",
    "        review = self.clean_stop_words(review)\n",
    "        review = self.correction_spelling(review)\n",
    "        \n",
    "        return review\n",
    "\n",
    "    def clean_dataset(self):\n",
    "        \"\"\"\n",
    "        Main method call to prepare a text to be vectorized.\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = self.replace_nan()\n",
    "        for i in trange(self.df.shape[0]):\n",
    "            review = self.df.at[i, 'avis']\n",
    "            review = self.clean_str(review)\n",
    "            review = self.clean_stop_words(review)\n",
    "            review = self.correction_spelling(review)\n",
    "                \n",
    "            self.df.at[i, 'avis'] = review\n",
    "\n",
    "    def filter_long_review(self):\n",
    "        \"\"\"\n",
    "        Filter the string with too many words.\n",
    "        \"\"\"\n",
    "\n",
    "        to_drop = []\n",
    "        for i in self.df.index:\n",
    "            review = self.df['avis'][i]\n",
    "            review_list = review.split()\n",
    "\n",
    "            if len(review_list) > self.max_words:\n",
    "                to_drop.append(i)\n",
    "\n",
    "        self.df = self.df.drop(to_drop)\n",
    "        print(f\"dropped {len(to_drop)} lines\")\n",
    "\n",
    "    def fix_repartition_for_4_classes(self):\n",
    "        \"\"\"\n",
    "        Fix the bad repartitions of the dataset, by removing randomly good reviews.\n",
    "        \"\"\"\n",
    "\n",
    "        d = self.df.groupby(['classe_bon_mauvais'], as_index=False).count()\n",
    "        nb_bad = d['avis'][0]\n",
    "\n",
    "        nb_good = d['avis'][2]\n",
    "        to_remove = nb_good - nb_bad\n",
    "\n",
    "        while(to_remove > 0):\n",
    "            row: DataFrame = self.df.sample()\n",
    "            index = row.first_valid_index()\n",
    "            print(f\"{to_remove}\")\n",
    "\n",
    "            if row['classe_bon_mauvais'][index] == 2:\n",
    "                self.df.drop(index, inplace=True)\n",
    "                to_remove -= 1\n",
    "\n",
    "\n",
    "        nb_good = d['avis'][3]\n",
    "        to_remove = nb_good - nb_bad\n",
    "\n",
    "        while(to_remove > 0):\n",
    "            row: DataFrame = self.df.sample()\n",
    "            index = row.first_valid_index()\n",
    "            print(f\"{to_remove}\")\n",
    "\n",
    "            if row['classe_bon_mauvais'][index] == 3:\n",
    "                self.df.drop(index, inplace=True)\n",
    "                to_remove -= 1\n",
    "        \n",
    "        d = self.df.groupby(['classe_bon_mauvais'], as_index=False).count()\n",
    "        nb_bad = d['avis'][0]\n",
    "        nb_good = d['avis'][1]\n",
    "        print(\"2\", nb_bad, nb_good)\n",
    "\n",
    "    def fix_repartition(self):\n",
    "        \"\"\"\n",
    "        Fix the bad repartitions of the dataset, by removing randomly good advice.\n",
    "        \"\"\"\n",
    "\n",
    "        d = self.df.groupby(['classe_bon_mauvais'], as_index=False).count()\n",
    "        nb_bad = d['avis'][0]\n",
    "        nb_good = d['avis'][1]\n",
    "        print(nb_bad, nb_good)\n",
    "\n",
    "        to_remove = nb_good - nb_bad\n",
    "        \n",
    "        while(to_remove > 0):\n",
    "            row: DataFrame = self.df.sample()\n",
    "            index = row.first_valid_index()\n",
    "            print(f\"{to_remove}\")\n",
    "\n",
    "            if row['classe_bon_mauvais'][index] == 1:\n",
    "                self.df.drop(index, inplace=True)\n",
    "                to_remove -= 1\n",
    "        \n",
    "        d = self.df.groupby(['classe_bon_mauvais'], as_index=False).count()\n",
    "        nb_bad = d['avis'][0]\n",
    "        nb_good = d['avis'][1]\n",
    "        print(\"2\", nb_bad, nb_good)\n",
    "\n",
    "    def save_data(self, path):\n",
    "        self.df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Pre-processing\n",
    "> Données avant pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aller, un bon 10 parce que ca reste jouable, mais systeme de paiement si tu veut plus, un jeu copier coller de l'ancienne version ,et on recommence ! pas besoin d'en dire plus\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/csv/dataset_original.csv')\n",
    "\n",
    "df['avis'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Données après pre-processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"aller bon parce reste jouable mais systeme paiement si veut plus copier coller l'ancienne version recommence pas besoin d'en dire plus\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/csv/dataset_0-1.csv')\n",
    "\n",
    "df['avis'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nous avons calculer les 30 mots les plus utilisés que l'on a par la suite enlevé. parmis eux :\n",
    "\n",
    "```\n",
    "\"c'est\", \"ces\", \"ses\", \"s'est\", \"a\", \"de\", \"du\", \"et\", \"le\", \"les\", \"un\", \"une\", \"pour\", \"sur\", \"etc\", \"est\", \"c\",\n",
    "'la', \"jeu\", \"que\", \"des\", \"en\", \"ce\", \"qu\", \"ca\", \"y\", \"je\", \"sa\", \"son\",\"au\", \"ai\", \"mon\", \"ma\", \"mes\", \"qui\", \"je\", \n",
    "\"tu\", \"il\", \"ils\", \"elles\", \"elle\", \"vous\", \"nous\",\"qu'il\", \"qu'elle\", \"qu'ils\", \"qu'elles\", \"qu'on\", \"on\", \"se\", \"par\"\n",
    "```\n",
    "\n",
    "> Les avis trop longs posent aussi problèmes. Certains avis comportaient plus de 1000 mots, pour une moyenne beaucoup plus basse. <br>\n",
    "> Cela pose problème pour l'entrainement mais aussi pour la représentation des avis.\n",
    "\n",
    "<br>\n",
    "\n",
    "> ## Classes des notes\n",
    "> > Pour représenter les notes, nous avons instaurer 2 classes différentes, 0 pour les notes < 12, et 1 pour les notes >= 12.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Dataset 2 classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classe_bon_mauvais</th>\n",
       "      <th>avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26642</th>\n",
       "      <td>0</td>\n",
       "      <td>j'ai acheté portal après avoir vu tous comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>1</td>\n",
       "      <td>très bon jeux moins bugé origin gigantesque op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>1</td>\n",
       "      <td>jeux parfaitement représente jeux vidéos moi t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0</td>\n",
       "      <td>pokemon honte vidéoludique avoir autant rabais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15477</th>\n",
       "      <td>0</td>\n",
       "      <td>j'ai créé compte jv juste poster commentaire j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23849</th>\n",
       "      <td>0</td>\n",
       "      <td>qu'est passé pourquoi avoir sorti grosse démo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>0</td>\n",
       "      <td>très bon avant c'était tuerie mais maintenant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11926</th>\n",
       "      <td>0</td>\n",
       "      <td>sérieusement l'ai emprunté pote ben garde dégo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32386</th>\n",
       "      <td>0</td>\n",
       "      <td>multijoueurs assez monotone campagne très liné...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31599</th>\n",
       "      <td>0</td>\n",
       "      <td>honte vidéo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35138 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       classe_bon_mauvais                                               avis\n",
       "26642                   0  j'ai acheté portal après avoir vu tous comment...\n",
       "3013                    1  très bon jeux moins bugé origin gigantesque op...\n",
       "3883                    1  jeux parfaitement représente jeux vidéos moi t...\n",
       "332                     0  pokemon honte vidéoludique avoir autant rabais...\n",
       "15477                   0  j'ai créé compte jv juste poster commentaire j...\n",
       "...                   ...                                                ...\n",
       "23849                   0  qu'est passé pourquoi avoir sorti grosse démo ...\n",
       "1361                    0  très bon avant c'était tuerie mais maintenant ...\n",
       "11926                   0  sérieusement l'ai emprunté pote ben garde dégo...\n",
       "32386                   0  multijoueurs assez monotone campagne très liné...\n",
       "31599                   0                                        honte vidéo\n",
       "\n",
       "[35138 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/csv/dataset_0-1.csv')\n",
    "\n",
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset 4 classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classe_bon_mauvais</th>\n",
       "      <th>avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>3</td>\n",
       "      <td>bon des très beaux graphismes pokémon les musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>2</td>\n",
       "      <td>je joue dofus depuis ans ne voit trés rarement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21932</th>\n",
       "      <td>2</td>\n",
       "      <td>il sera fait attendre titre malgré tout après ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>3</td>\n",
       "      <td>je suis désolé vraiment mais fait partie très ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23458</th>\n",
       "      <td>0</td>\n",
       "      <td>encore réchauffé encore toujours pouriiiiiiiii...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27459</th>\n",
       "      <td>0</td>\n",
       "      <td>n'achetez surtout pas ce jeu j'ai testé car pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16025</th>\n",
       "      <td>2</td>\n",
       "      <td>les</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8035</th>\n",
       "      <td>3</td>\n",
       "      <td>oh mais qu'est j'en marre compare deux jeux so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22179</th>\n",
       "      <td>0</td>\n",
       "      <td>tout négatif dans bugs impossibilité choisir c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>3</td>\n",
       "      <td>tout d'abord tiens dire changement studio halo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       classe_bon_mauvais                                               avis\n",
       "109                     3  bon des très beaux graphismes pokémon les musi...\n",
       "1305                    2  je joue dofus depuis ans ne voit trés rarement...\n",
       "21932                   2  il sera fait attendre titre malgré tout après ...\n",
       "14589                   3  je suis désolé vraiment mais fait partie très ...\n",
       "23458                   0  encore réchauffé encore toujours pouriiiiiiiii...\n",
       "...                   ...                                                ...\n",
       "27459                   0  n'achetez surtout pas ce jeu j'ai testé car pe...\n",
       "16025                   2                                                les\n",
       "8035                    3  oh mais qu'est j'en marre compare deux jeux so...\n",
       "22179                   0  tout négatif dans bugs impossibilité choisir c...\n",
       "2159                    3  tout d'abord tiens dire changement studio halo...\n",
       "\n",
       "[29748 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/csv/dataset_0-3.csv')\n",
    "\n",
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Corretions des mots\n",
    "> > Afin de tenir comptes des mots mal orthographiés, nous avons utilisés la librairie [spellchecker](https://pypi.org/project/pyspellchecker/). <br>\n",
    "> > Malheureusement nous avons fait une erreur dans le code. En effet nous avons transformé tous les points et toutes les virgules par du vide. <br>\n",
    "> > Nous avons perdu par ce biais beaucoup de mots car inutilisables pour la méthode Word2Vec.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Base line:\n",
    ">\n",
    "> > #### Prédicteurs de base:\n",
    "> > > Pour commencer notre projet, nous avons fait deux systèmes de prédictions simples. <br>\n",
    "> > > De base, une classe est assignée à chaque avis. Si la note attribuée est supérieure ou égale à 12 alors on assigne l'avis à la classe 1 comme signe de bon jeu. <br>\n",
    "> > > En revanche, si la note est strictement inférieure à 12 alors, l'avis est catégorisé par la classe 0, celle des mauvais jeux. <br>\n",
    "> > > ##### Prédicteur par comptage de mots:\n",
    "> > > L'idée ici est plutôt naïve. Si le commentaire contient plus de mots au sens négatifs, alors on prédit le jeu comme étant mauvais. À l'inverse, <br>\n",
    "> > > si la condition énoncée n'est pas validée alors le jeu est considéré comme étant bon. <br>\n",
    "> > > Le code marche comme ceci:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102478/102478 [00:09<00:00, 10778.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15110, 4847], [48369, 34152]]\n",
      "cat 0: TP:15110, TN:64372, FP:48369, FN:4847\n",
      "\n",
      "cat 1: TP:34152, TN:83414, FP:4847, FN:48369\n",
      "\n",
      "accuracy: 0.64368358122073\n",
      "\n",
      "recall: 0.585493057720152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "\n",
    "def predict(data: pd.DataFrame) -> tuple[list[int], list[int]]:\n",
    "    classes_predicted = []\n",
    "    classes_base = []\n",
    "\n",
    "    for i in trange(data.shape[0]):\n",
    "        row = data.iloc[i]\n",
    "        neg = row['negative_words']\n",
    "        pos = row['positive_words']\n",
    "        \n",
    "        predict = 1\n",
    "        if neg > pos: \n",
    "            predict = 0\n",
    "        \n",
    "        base = row['classe_bon_mauvais']\n",
    "        classes_base.append(int(base))\n",
    "        classes_predicted.append(int(predict))\n",
    "    \n",
    "    return classes_base, classes_predicted\n",
    "    \n",
    "def compute_confusion_matrix(classes_base, classes_predicted):\n",
    "    \n",
    "    M = [[0, 0], [0, 0]]\n",
    "\n",
    "    for i in range(len(classes_base)):\n",
    "        M[classes_base[i]][classes_predicted[i]] += 1\n",
    "    \n",
    "    return M\n",
    "\n",
    "def accuracy(TN, TP, FN, FP):\n",
    "    size_list = len(TN)\n",
    "    accuracy_sum = 0\n",
    "    for i in range(size_list):\n",
    "        accuracy_sum += (TP[i] + TN[i]) / (TP[i] + TN[i] + FN[i] + FP[i])\n",
    "    return accuracy_sum / size_list\n",
    "\n",
    "def recall(TP, FN):\n",
    "    size_list = len(TP)\n",
    "    recall_sum = 0\n",
    "    for i in range(size_list):\n",
    "        recall_sum += TP[i] / (TP[i] + FN[i])\n",
    "    return recall_sum / size_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv('dataset/csv/base_predictor.csv')\n",
    "    classes_base, classes_predicted = predict(data)\n",
    "    M = compute_confusion_matrix(classes_base, classes_predicted)\n",
    "    print(M)\n",
    "    TP = [0,0]\n",
    "    TN = [0,0]\n",
    "    FP = [0,0]\n",
    "    FN = [0,0]\n",
    "    Total = M[0][0] + M[0][1] + M[1][0] + M[1][1]\n",
    "\n",
    "    for i in range(2):\n",
    "        TP[i] = M[i][i]\n",
    "        for j in range(2):\n",
    "            FN[i] += M[i][j]\n",
    "            FP[i] += M[j][i]\n",
    "        \n",
    "        FN[i] -= M[i][i]\n",
    "        FP[i] -= M[i][i]\n",
    "        TN[i] = Total - FP[i] - FN[i] + TP[i]\n",
    "\n",
    "    for i in range(2):\n",
    "        print(f\"cat {i}: TP:{TP[i]}, TN:{TN[i]}, FP:{FP[i]}, FN:{FN[i]}\\n\")\n",
    "\n",
    "    print (f\"accuracy: {accuracy(TN, TP, FN, FP)}\\n\")\n",
    "    print (f\"recall: {recall(TP, FN)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> > > ##### Prédicteur toujours bon:\n",
    "> > > L'idée ici est plus que simple. Peu importe que la classe de base soit bonne ou mauvaise, on prédit toujours que le jeu est bon. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 0: TP:0, TN:82521, FP:0, FN:19957\n",
      "\n",
      "cat 1: TP:82521, TN:165042, FP:19957, FN:0\n",
      "\n",
      "the globla accuracy is 0.87\n",
      "\n",
      "the globla recall is 0.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_class(data): #here we will assign a class number according to the rate. 1 for good games and 2 for bad games.\n",
    "    classes_predicted = []\n",
    "    classes_base = []\n",
    "    predict = 1\n",
    "    for i in range(data.shape[0]):\n",
    "        row = data.iloc[i]\n",
    "        base = row['classe_bon_mauvais']\n",
    "        classes_base.append(int(base))\n",
    "        classes_predicted.append(int(predict))\n",
    "    return classes_base, classes_predicted\n",
    "\n",
    "def accuracy(TN, TP, FN, FP):\n",
    "    size_list = len(TN)\n",
    "    accuracy_sum = 0\n",
    "    for i in range(size_list):\n",
    "        accuracy_sum += (TP[i] + TN[i]) / (TP[i] + TN[i] + FN[i] + FP[i])\n",
    "    return accuracy_sum / size_list\n",
    "\n",
    "def recall(TP, FN):\n",
    "    size_list = len(TP)\n",
    "    recall_sum = 0\n",
    "    for i in range(size_list):\n",
    "        recall_sum += TP[i] / (TP[i] + FN[i])\n",
    "    return recall_sum / size_list\n",
    "\n",
    "def CM(classes_base, classes_predicted):\n",
    "    M = [[0, 0], [0, 0]]\n",
    "\n",
    "    for i in range(len(classes_base)):\n",
    "        M[classes_base[i]][classes_predicted[i]] += 1\n",
    "\n",
    "    TP = [0,0]\n",
    "    TN = [0,0]\n",
    "    FP = [0,0]\n",
    "    FN = [0,0]\n",
    "    Total = M[0][0] + M[0][1] + M[1][0] + M[1][1]\n",
    "\n",
    "    for i in range(2):\n",
    "        TP[i] = M[i][i]\n",
    "        for j in range(2):\n",
    "            FN[i] += M[i][j]\n",
    "            FP[i] += M[j][i]\n",
    "        \n",
    "        FN[i] -= M[i][i]\n",
    "        FP[i] -= M[i][i]\n",
    "        TN[i] = Total - FP[i] - FN[i] + TP[i]\n",
    "\n",
    "    for i in range(2):\n",
    "        print(f\"cat {i}: TP:{TP[i]}, TN:{TN[i]}, FP:{FP[i]}, FN:{FN[i]}\\n\")\n",
    "\n",
    "    accu = accuracy(TN, TP, FN, FP)\n",
    "    rec = recall(TP, FN)\n",
    "\n",
    "    print(f\"the globla accuracy is {accu:.2f}\\n\")\n",
    "    print(f\"the globla recall is {rec:.2f}\\n\")\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    my_data = pd.read_csv(\"./dataset/csv/base_predictor.csv\")\n",
    "    classes_base, classes_predicted = predict_class(my_data)\n",
    "    CM(classes_base, classes_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## CLassifiers Classes\n",
    "> > #### Classifier Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import src.utils.utils as u\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix)\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "class ClassifierType(Enum):\n",
    "    WORD2VEC = 1\n",
    "    NAIVES_BAYES = 2\n",
    "    WORD2VEC_MIX = 3\n",
    "    TFIDF_MNB = 4\n",
    "    TFIDF_LogReg = 5\n",
    "    TFIDF_MLP = 6\n",
    "    \n",
    "    \n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "    Model Class of all Classifiers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data) -> None:\n",
    "        self.data = data\n",
    "        self.verify_data()\n",
    "    \n",
    "        self.classifier = None\n",
    "        self.predictions = None\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "    \n",
    "    def verify_data(self):\n",
    "        print(\"Verifying data ... \")\n",
    "        to_drop = []\n",
    "        for i in self.data.index:\n",
    "            r = self.data['avis'][i]\n",
    "            if pd.isna(r) or r in ['nan', 'Nan'] or type(r) == float:\n",
    "                to_drop.append(i)\n",
    "        print(\"droped nan : \", len(to_drop))\n",
    "        self.data = self.data.drop(to_drop)\n",
    "    \n",
    "    def show_repartition(self) -> None:\n",
    "        print(self.data.groupby(['classe_bon_mauvais'], as_index=False).count())\n",
    "    \n",
    "    def save(self, path, features = None):\n",
    "        dump(self.classifier, path)\n",
    "    \n",
    "    def load(self, model_path: str, features_path: str = None):\n",
    "        self.classifier = load(model_path)\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"Training on data...\")\n",
    "        self.classifier.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def predict(self):\n",
    "        print(\"Prediciton on tests...\")\n",
    "        self.predictions = self.classifier.predict(self.X_test)\n",
    "\n",
    "    def show_results(self):\n",
    "        print('==========================Classifier Results============================')\n",
    "        M = confusion_matrix(self.y_test, self.predictions)\n",
    "        print(M)\n",
    "\n",
    "        print('\\n Accuracy: ', accuracy_score(self.y_test, self.predictions))\n",
    "        print('\\n Score: ', self.classifier.score(self.X_test, self.y_test))\n",
    "\n",
    "        print(u.compute_metrics(2, M))\n",
    "        print(classification_report(self.y_test, self.predictions))\n",
    "\n",
    "    # Abstract methods\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        return accuracy_score(self.y_test, self.predictions)\n",
    "\n",
    "    def get_precisions(self, c):\n",
    "        d = classification_report(self.y_test, self.predictions, output_dict=True)\n",
    "        c_str = str(c)\n",
    "\n",
    "        return d[c_str]['precision']\n",
    "\n",
    "\n",
    "    def plot_matrix_classification_report(self, title, cp_path, matrix_path, classes):\n",
    "        y_test = self.y_test\n",
    "        predic = self.predictions\n",
    "\n",
    "        confm = confusion_matrix(y_test, predic)\n",
    "        df_cm = pd.DataFrame(confm, index=classes, columns=classes)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12,10))\n",
    "        ax.set_title('Confusion matrix for '+ title)\n",
    "        sb.heatmap(df_cm, cmap='YlOrRd', annot=True, fmt='g', ax=ax)\n",
    "        plt.savefig(matrix_path)\n",
    "\n",
    "        c_r = classification_report(self.y_test, self.predictions)\n",
    "        f = open(cp_path, 'a')\n",
    "        f.write(c_r)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "\n",
    "    def plot_accuracy_precisions(self, title, acc_path, prec_path, label, params, classes: list[int], accuracies: list[int], precisions: list[list[int]]):\n",
    "        \n",
    "        # Accuracy\n",
    "        plt.title('Accuracy for ' + title)\n",
    "        plt.xlabel(label)\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.plot(params, accuracies)\n",
    "        plt.savefig(acc_path)\n",
    "        plt.show()\n",
    "\n",
    "        # Precisions\n",
    "        plt.title('Precisions for ' + title)\n",
    "        plt.xlabel(label)\n",
    "        plt.ylabel('precisions')\n",
    "        \n",
    "        color = ['r', 'b', 'g', 'y']\n",
    "        for i in range(len(precisions)):\n",
    "            plt.plot(params, precisions[i], color=color[i], label=classes[i])\n",
    "        plt.legend(loc=\"upper right\", title='classes')\n",
    "\n",
    "        plt.savefig(prec_path)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_transform_data(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def init_sets(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def init_classifier(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict_input(self, review: str):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## PipeLine des Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineClassifier:\n",
    "\n",
    "    def __init__(self, classifier_type, data, vec_bin=None, max_features=None, nb_word_n=None, max_word=None, max_iter=None, test_size=None, layers=None, vec_dim=None, reg=None, alpha=None) -> None:\n",
    "        self.classifier = None\n",
    "        self.classifier_type = classifier_type\n",
    "        self.data = data\n",
    "        self.max_features = max_features\n",
    "        self.vec_bin = vec_bin\n",
    "        self.nb_word_n = nb_word_n\n",
    "        self.max_word = max_word\n",
    "        self.max_iter = max_iter\n",
    "        self.test_size = test_size\n",
    "        self.layers = layers\n",
    "        self.vec_dim = vec_dim\n",
    "        self.reg = reg\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.__init_classifier()\n",
    "\n",
    "    def __init_classifier(self):\n",
    "        if self.classifier_type == ClassifierType.WORD2VEC:\n",
    "            self.classifier = ClassifierWord2Vec(\n",
    "                data=self.data,\n",
    "                word2vec_bin=self.vec_bin,\n",
    "                max_iter=self.max_iter,\n",
    "                layers=self.layers,\n",
    "                vec_dim=self.vec_dim,\n",
    "                test_size=self.test_size\n",
    "            )\n",
    "\n",
    "        elif self.classifier_type == ClassifierType.NAIVES_BAYES:\n",
    "            self.classifier = NaivesBayes(\n",
    "                data=self.data,\n",
    "                nb_word=self.nb_word_n,\n",
    "                test_size=self.test_size\n",
    "            )\n",
    "\n",
    "        elif self.classifier_type == ClassifierType.WORD2VEC_MIX:\n",
    "            self.classifier = ClassifierWord2VecMix(\n",
    "                data=self.data,\n",
    "                word2vec_bin=self.vec_bin,\n",
    "                max_iter=self.max_iter,\n",
    "                layers=self.layers,\n",
    "                vec_dim=self.vec_dim,\n",
    "                test_size=self.test_size\n",
    "            )\n",
    "        \n",
    "        elif self.classifier_type == ClassifierType.TFIDF_LogReg:\n",
    "            self.classifier = TFIDF_LogReg(\n",
    "                data=self.data,\n",
    "                test_size=self.test_size,\n",
    "                max_iter=self.max_iter,\n",
    "                regularization=self.reg,\n",
    "                max_features=self.max_features\n",
    "            )\n",
    "\n",
    "        elif self.classifier_type == ClassifierType.TFIDF_MNB:\n",
    "            self.classifier = TFIDF_MNB(\n",
    "                data=self.data,\n",
    "                test_size=self.test_size,\n",
    "                alpha=self.alpha,\n",
    "                max_features=self.max_features\n",
    "            )\n",
    "        elif self.classifier_type == ClassifierType.TFIDF_MLP:\n",
    "            self.classifier = TFIDF_MLP(\n",
    "                data=self.data,\n",
    "                test_size=self.test_size,\n",
    "                max_iter=self.max_iter,\n",
    "                layers=self.layers,\n",
    "                max_features=self.max_features\n",
    "            )\n",
    "            \n",
    "    \n",
    "    def load(self, model_path, features_path=None):\n",
    "        self.classifier.load(model_path, features_path)\n",
    "        self.classifier.init_sets()\n",
    "        self.classifier.X_train = None\n",
    "        self.classifier.y_train = None\n",
    "        self.classifier.fit_transform_data()\n",
    "\n",
    "    def save(self, model_path, features_path=None):\n",
    "        self.classifier.save(model_path, features_path)\n",
    "\n",
    "    def train(self):\n",
    "        self.classifier.init_sets()\n",
    "        self.classifier.init_classifier()\n",
    "        self.classifier.fit_transform_data()\n",
    "        self.classifier.train()\n",
    "\n",
    "    def transform_data(self):\n",
    "        self.classifier.init_sets()\n",
    "        self.classifier.fit_transform_data()\n",
    "    \n",
    "    def train_without_transform(self):\n",
    "        self.classifier.train()\n",
    "    \n",
    "    def predict(self):\n",
    "        self.classifier.show_repartition()\n",
    "        self.classifier.predict()\n",
    "        self.classifier.show_results()\n",
    "\n",
    "    def predict_input(self):\n",
    "        review = input(\"Write a review to predict: \\n\")\n",
    "        c = CleanData(self.max_word)\n",
    "        review = c.clean_review(review)\n",
    "        review = self.classifier.predict_input(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Classifier Word2Vec Multi-layer Perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from src.utils.utils import compute_metrics\n",
    "from src.classifiers.Classifier import Classifier\n",
    "\n",
    "\n",
    "class ClassifierWord2Vec(Classifier):\n",
    "\n",
    "    def __init__(self, data, word2vec_bin = None, max_iter=0, test_size=0, layers=0, vec_dim=0, create = True) -> None:\n",
    "        super().__init__(data)\n",
    "        \n",
    "        self.max_iter = max_iter\n",
    "        self.test_size = test_size\n",
    "        self.layers = layers\n",
    "        self.vec_dim = vec_dim\n",
    "        self.word2vec_bin = word2vec_bin\n",
    "        \n",
    "    \n",
    "        self.words_dictionary = None\n",
    "\n",
    "        self.load_dictionnary_from_bin()\n",
    "\n",
    "    def load_dictionnary_from_bin(self):\n",
    "        print(f\"Loading dictionary from binary {self.word2vec_bin}...\")\n",
    "        self.words_dictionnary: KeyedVectors = KeyedVectors.load_word2vec_format(self.word2vec_bin, binary=True)\n",
    "\n",
    "    def init_sets(self):\n",
    "        print(\"Initialization of train and test sets...\")\n",
    "        X = self.data['avis'].copy()\n",
    "        y = self.data['classe_bon_mauvais'].copy()\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size)\n",
    "    \n",
    "    def init_classifier(self):\n",
    "        print(f\"Initialization of the MLP Classifier with {self.layers} layers and {self.max_iter} max iteration.\")\n",
    "        \n",
    "        self.classifier = MLPClassifier(hidden_layer_sizes=self.layers, max_iter=self.max_iter)\n",
    "\n",
    "    def _transform_to_vec(self, set_to_transform):\n",
    "        vectors_list = []\n",
    "        len_set = len(set_to_transform)\n",
    "        acc = 1\n",
    "        for review in set_to_transform:\n",
    "            print(f\"{acc}/{len_set}\", end='\\r')\n",
    "            acc+=1\n",
    "            try:\n",
    "                reviews_list = review.split()\n",
    "            except:\n",
    "                print('exception !', review)\n",
    "                if (type(review) == float):\n",
    "                    print(type(review))\n",
    "                exit()\n",
    "            reviews_vec = []\n",
    "            \n",
    "            for word in reviews_list:\n",
    "                try:\n",
    "                    word_vec = self.words_dictionnary[word]\n",
    "                    reviews_vec.append(word_vec)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for i in range(len(reviews_vec), self.vec_dim):\n",
    "                reviews_vec.append(np.zeros(self.vec_dim, dtype=np.float32))\n",
    "\n",
    "            tot_vec = []\n",
    "            for i in range(self.vec_dim):\n",
    "                sum = 0.0\n",
    "                for vec in reviews_vec[i]:\n",
    "                    sum += vec\n",
    "                tot_vec.append(sum)\n",
    "\n",
    "            vectors_list.append(tot_vec)\n",
    "        \n",
    "        return vectors_list\n",
    "\n",
    "    def fit_transform_data(self):\n",
    "        if self.X_test is not None:\n",
    "            print(f\"Transforming reviews of the test set into vectors...\")\n",
    "            self.X_test = self._transform_to_vec(self.X_test)\n",
    "        \n",
    "        if self.X_train is not None:\n",
    "            print(f\"Transforming reviews of the train set into vectors...\")\n",
    "            self.X_train = self._transform_to_vec(self.X_train)\n",
    "        \n",
    "    def train(self):\n",
    "        print(\"Training on data...\")\n",
    "        self.classifier.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def predict(self):\n",
    "        print(\"Prediciton on tests...\")\n",
    "        self.predictions = self.classifier.predict(self.X_test)\n",
    "\n",
    "    def show_results(self):\n",
    "        M = confusion_matrix(self.y_test, self.predictions)\n",
    "\n",
    "        print(M)\n",
    "        print(compute_metrics(2, M))\n",
    "\n",
    "        print(classification_report(self.y_test, self.predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Classifier Word2Vec Multi-layer Perceptron classifier mélange de model pré-entrainé et auto-entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import DefaultDict\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from src.classifiers.Classifier import Classifier\n",
    "\n",
    "from src.classifiers.ClassifierWord2Vec import ClassifierWord2Vec\n",
    "from gensim.models import FastText  \n",
    "\n",
    "\n",
    "class ClassifierWord2VecMix(ClassifierWord2Vec):\n",
    "\n",
    "    def __init__(self, data, word2vec_bin=None, max_word=0, max_iter=0, test_size=0, layers=0, vec_dim=0) -> None:\n",
    "        super().__init__(\n",
    "            data,\n",
    "            word2vec_bin=word2vec_bin,\n",
    "            max_word=max_word,\n",
    "            max_iter=max_iter,\n",
    "            test_size=test_size,\n",
    "            layers=layers,\n",
    "            vec_dim=vec_dim\n",
    "            )\n",
    "\n",
    "        self.words_dictionary_self = None\n",
    "        \n",
    "        self.create_dictionnary()\n",
    "\n",
    "    def create_dictionnary(self):\n",
    "        sentences = [s.split() for s in self.data['avis']]\n",
    "\n",
    "        print(\"Training model\")\n",
    "        model = FastText(sentences=sentences, vector_size=self.vec_dim)\n",
    "        print('Training done')\n",
    "        self.words_dictionary_self = model.wv\n",
    "\n",
    "    def _transform_to_vec(self, set_to_transform):\n",
    "        vectors_list = []\n",
    "        len_set = len(set_to_transform)\n",
    "        acc = 1\n",
    "        for review in set_to_transform:\n",
    "            print(f\"{acc}/{len_set}\", end='\\r')\n",
    "            acc+=1\n",
    "            reviews_list = review.split()\n",
    "            reviews_vec = []\n",
    "            \n",
    "            for word in reviews_list:\n",
    "                try:\n",
    "                    word_vec = self.words_dictionnary[word]\n",
    "                    reviews_vec.append(word_vec)\n",
    "                except:\n",
    "                    word_vec = self.words_dictionary_self[word]\n",
    "                    reviews_vec.append(word_vec)\n",
    "\n",
    "            for i in range(len(reviews_vec), self.vec_dim):\n",
    "                reviews_vec.append(np.zeros(self.vec_dim, dtype=np.float32))\n",
    "\n",
    "            tot_vec = []\n",
    "            for i in range(self.vec_dim):\n",
    "                sum = 0.0\n",
    "                for vec in reviews_vec[i]:\n",
    "                    sum += vec\n",
    "                tot_vec.append(sum)\n",
    "\n",
    "            vectors_list.append(tot_vec)\n",
    "        \n",
    "        return vectors_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Classifier Word Features with Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.classifiers.Classifier import Classifier\n",
    "from src.utils.clean_data import CleanData\n",
    "from joblib import dump, load\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "class NaivesBayes(Classifier):\n",
    "\n",
    "\n",
    "    def __init__(self, data, nb_word, test_size=0) -> None:\n",
    "        super().__init__(data)\n",
    "\n",
    "        self.test_size = test_size\n",
    "\n",
    "        self.train_set = None\n",
    "        self.test_set = None\n",
    "\n",
    "        self.nb_word = nb_word\n",
    "\n",
    "        self.refsets = None\n",
    "        self.testsets = None\n",
    "        self.predictions_labels = None\n",
    "        self.predictions = None\n",
    "\n",
    "        self.word_features = None\n",
    "    \n",
    "    def _review_features(self, review):\n",
    "        review_words = set(review)\n",
    "        features = {}\n",
    "        for word in self.word_features:\n",
    "            features['contains({})'.format(word)] = (word in review_words)\n",
    "        return features\n",
    "\n",
    "    def _compute_word_features(self):\n",
    "        all_words = nltk.FreqDist()\n",
    "    \n",
    "        print(\"Computes words frequencies ...\")\n",
    "        acc = 0\n",
    "        size = self.data.shape[0]\n",
    "        for avis in self.data['avis']:\n",
    "            acc+=1\n",
    "            print(acc, \"/\", size, sep='', end='\\r')\n",
    "            for word in avis.split():\n",
    "                all_words[word] += 1\n",
    "\n",
    "        self.word_features = list(all_words)[:self.nb_word]\n",
    "\n",
    "    def _compute_features(self, tuple_to_compute):\n",
    "        print(\"get Features out of review ...\")\n",
    "        \n",
    "        featuresets = []\n",
    "        acc = 0\n",
    "        size = len(tuple_to_compute)\n",
    "        for (r, c) in tuple_to_compute:\n",
    "            acc+=1\n",
    "            print(acc, \"/\", size, sep='', end='\\r')\n",
    "            featuresets.append((self._review_features(r), c))\n",
    "        \n",
    "        return featuresets\n",
    "\n",
    "    def _get_tuples(self, input, output):\n",
    "        print(\"Get all tuples of reviews ...\")\n",
    "        reviews = []\n",
    "        size = len(input)\n",
    "        for i in input.index:\n",
    "            print(i, \"/\", size, sep='', end='\\r')\n",
    "            reviews.append((input[i].split(), output[i]))\n",
    "\n",
    "        return reviews\n",
    "\n",
    "    def load(self, model_path: str, features_path: str = None):\n",
    "        self.classifier = load(model_path)\n",
    "\n",
    "        with open(features_path, 'r', encoding='utf8') as fp:\n",
    "            data = json.load(fp)\n",
    "            self.word_features = data['features']\n",
    "\n",
    "    def save(self, path, features = None):\n",
    "        data = {}\n",
    "        data['features'] = self.word_features\n",
    "        dump(self.classifier, path)\n",
    "        with open(features, 'w', encoding='utf8') as fp:\n",
    "            json.dump(data, fp, indent=4, ensure_ascii=False)\n",
    "\n",
    "    def init_classifier(self):\n",
    "        print(\"Initialization of the word features dictionnary\")\n",
    "        self._compute_word_features()\n",
    "\n",
    "    def init_sets(self):\n",
    "        print(\"Initialization of train and test sets...\")\n",
    "        X = self.data['avis'].copy()\n",
    "        y = self.data['classe_bon_mauvais'].copy()\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size)\n",
    "        \n",
    "    def fit_transform_data(self):\n",
    "        if self.X_test is not None:\n",
    "            print(f\"Transforming reviews of the test set into tuples of features...\")\n",
    "            self.test_set = self._get_tuples(self.X_test, self.y_test)\n",
    "            self.test_set = self._compute_features(self.test_set)\n",
    "        \n",
    "        if self.X_train is not None:\n",
    "            print(f\"Transforming reviews of the train set into tuples of features...\")\n",
    "            self.train_set = self._get_tuples(self.X_train, self.y_train)\n",
    "            self.train_set = self._compute_features(self.train_set)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Training on data...\")\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(self.train_set)\n",
    "\n",
    "    def predict(self):\n",
    "        print(\"Prediciton on tests...\")\n",
    "\n",
    "        self.predictions = []\n",
    "        self.predictions_labels = []\n",
    "        self.refsets = collections.defaultdict(set)\n",
    "        self.testsets = collections.defaultdict(set)\n",
    "        \n",
    "        for i, (feats, label) in enumerate(self.test_set):\n",
    "            print(f\"{i}/{len(self.test_set)}\", end='\\r')\n",
    "            self.refsets[label].add(i)\n",
    "            observed = self.classifier.classify(feats)\n",
    "            self.testsets[observed].add(i)\n",
    "            self.predictions_labels.append(label)\n",
    "            self.predictions.append(observed)\n",
    "    \n",
    "    def show_results(self):\n",
    "        print(\"Confusion Matrix:\\n\", nltk.ConfusionMatrix(self.predictions_labels, self.predictions))\n",
    "        print(\"accuracy:\", nltk.accuracy(self.predictions_labels, self.predictions))\n",
    "        \n",
    "        for i in range(2):\n",
    "            print (f\"class {i}:\")\n",
    "            print(\"f1_score:\", nltk.f_measure(self.refsets[i], self.testsets[i]))\n",
    "            print(\"recall:\", nltk.recall(self.refsets[i], self.testsets[i]))\n",
    "            print(\"precision:\", nltk.precision(self.refsets[i], self.testsets[i]))\n",
    "\n",
    "    def predict_input(self, review: str):\n",
    "        test = self._review_features(review.split())\n",
    "        print(self.classifier.classify(test))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Classifier TF-IDF avec Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classifiers.Classifier import Classifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "class TFIDF_LogReg(Classifier):\n",
    "\n",
    "    def __init__(self, data, test_size, max_iter, regularization, max_features) -> None:\n",
    "        super().__init__(data)\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.max_iter = max_iter\n",
    "        self.regularization = regularization\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def init_sets(self):\n",
    "        print(\"Initialization of train and test sets...\")\n",
    "        td = TfidfVectorizer(max_features=self.max_features) \n",
    "        X = self.data['avis'].copy()\n",
    "        X = td.fit_transform(X).toarray()\n",
    "        y = self.data['classe_bon_mauvais'].copy()\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size, random_state=0)\n",
    "\n",
    "    def init_classifier(self):\n",
    "        print(f\"Initialization of the LogReg Classifier with a reg of {self.regularization} and {self.max_iter} max iteration.\")\n",
    "        self.classifier = LogisticRegression(C=self.regularization, max_iter=self.max_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Classifier TF-IDF avec Multinomial Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classifiers.Classifier import Classifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "class TFIDF_MNB(Classifier):\n",
    "\n",
    "    def __init__(self, data, test_size, alpha, max_features) -> None:\n",
    "        super().__init__(data)\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def init_sets(self):\n",
    "        print(\"Initialization of train and test sets...\")\n",
    "        td = TfidfVectorizer(max_features=self.max_features) \n",
    "        X = self.data['avis'].copy()\n",
    "        X = td.fit_transform(X).toarray()\n",
    "        y = self.data['classe_bon_mauvais'].copy()\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size, random_state=0)\n",
    "\n",
    "    def init_classifier(self):\n",
    "        print(f\"Initialization of the LogReg Classifier with a alpha of {self.alpha}.\")\n",
    "        self.classifier = MultinomialNB(alpha=self.alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Classifier TF-IDF avec Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classifiers.Classifier import Classifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "class TFIDF_MLP(Classifier):\n",
    "\n",
    "    def __init__(self, data, test_size, max_iter, layers, max_features) -> None:\n",
    "        super().__init__(data)\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.max_iter = max_iter\n",
    "        self.layers = layers\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def init_sets(self):\n",
    "        print(\"Initialization of train and test sets...\")\n",
    "        td = TfidfVectorizer(max_features=self.max_features) \n",
    "        X = self.data['avis'].copy()\n",
    "        X = td.fit_transform(X).toarray()\n",
    "        y = self.data['classe_bon_mauvais'].copy()\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size, random_state=0)\n",
    "\n",
    "    def init_classifier(self):\n",
    "        #print(f\"Initialization of the LogReg Classifier with a reg of {self.regularization} and {self.max_iter} max iteration.\")\n",
    "        self.classifier = MLPClassifier(hidden_layer_sizes=self.layers, max_iter=self.max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Expérimentations\n",
    "> > Nous avons effectué plusieurs expérimentations afin de trouver les meilleurs paramètres de chaque modèle.\n",
    "\n",
    "<br>\n",
    "\n",
    "> ## Différences entre les datasets:\n",
    "> > Pour les expériences suivantes nous utiliserons le modèles MultinomialNB.\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Performances sur le dataset original :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_SIZE = 1/3\n",
    "ALPHA = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_MNB\n",
    "\n",
    "DATASET = 'dataset/csv/data_original.csv'\n",
    "\n",
    "PLOT_MATRIX_PATH = 'assets/data_analysis/data_original.plot.png'\n",
    "CP_PATH = 'assets/data_analysis/data_original_cp.txt'\n",
    "\n",
    "TILE_CM = 'data_original_CM'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, alpha=ALPHA, max_features=MAX_FEATURES)\n",
    "\n",
    "p.train()\n",
    "p.predict()\n",
    "\n",
    "p.classifier.plot_matrix_classification_report(TILE_CM, CP_PATH, PLOT_MATRIX_PATH, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Results :\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.37      0.52      5758\n",
    "           1       0.89      0.99      0.93     28402\n",
    "\n",
    "    accuracy                           0.89     34160\n",
    "   macro avg       0.88      0.68      0.73     34160\n",
    "weighted avg       0.88      0.89      0.87     34160\n",
    "```\n",
    "\n",
    "\n",
    "<img src='assets\\data_analysis\\data_original.plot.png' width='750'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Performances sur le dataset original avec répartition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_SIZE = 1/3\n",
    "ALPHA = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_MNB\n",
    "\n",
    "DATASET = 'dataset/csv/data_original_rep.csv'\n",
    "\n",
    "PLOT_MATRIX_PATH = 'assets/data_analysis/data_original_rep.plot.png'\n",
    "CP_PATH = 'assets/data_analysis/data_original_rep_cp.txt'\n",
    "\n",
    "TILE_CM = 'data_original_rep_CM'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, alpha=ALPHA, max_features=MAX_FEATURES)\n",
    "\n",
    "p.train()\n",
    "p.predict()\n",
    "\n",
    "p.classifier.plot_matrix_classification_report(TILE_CM, CP_PATH, PLOT_MATRIX_PATH, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Results :\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.85      0.85      0.85      5838\n",
    "           1       0.85      0.85      0.85      5875\n",
    "\n",
    "    accuracy                           0.85     11713\n",
    "   macro avg       0.85      0.85      0.85     11713\n",
    "weighted avg       0.85      0.85      0.85     11713\n",
    "```\n",
    "\n",
    "<img src='assets/data_analysis/data_original_rep.plot.png' width='750'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Performances sur le dataset original en nettoyant les caractères spéciaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_SIZE = 1/3\n",
    "ALPHA = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_MNB\n",
    "DATASET = 'dataset/csv/data_clean_str.csv'\n",
    "\n",
    "\n",
    "PLOT_MATRIX_PATH = 'assets/data_analysis/data_clean_str.plot.png'\n",
    "CP_PATH = 'assets/data_analysis/data_clean_str_cp.txt'\n",
    "TILE_CM = 'data_clean_str_CM'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, alpha=ALPHA, max_features=MAX_FEATURES)\n",
    "\n",
    "p.train()\n",
    "p.predict()\n",
    "\n",
    "p.classifier.plot_matrix_classification_report(TILE_CM, CP_PATH, PLOT_MATRIX_PATH, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Results :\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.85      0.85      0.85      5838\n",
    "           1       0.85      0.85      0.85      5875\n",
    "\n",
    "    accuracy                           0.85     11713\n",
    "   macro avg       0.85      0.85      0.85     11713\n",
    "weighted avg       0.85      0.85      0.85     11713\n",
    "```\n",
    "\n",
    "<img src='assets/data_analysis/data_clean_str.plot.png' width='750'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Performances sur le dataset original en nettoyant les \"stop words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_SIZE = 1/3\n",
    "ALPHA = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_MNB\n",
    "DATASET = 'dataset/csv/data_clean_str_stop_words.csv'\n",
    "\n",
    "PLOT_MATRIX_PATH = 'assets/data_analysis/data_clean_str_stop_words.plot.png'\n",
    "CP_PATH = 'assets/data_analysis/data_clean_str_stop_words_cp.txt'\n",
    "TILE_CM = 'data_clean_str_stop_words_CM'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, alpha=ALPHA, max_features=MAX_FEATURES)\n",
    "\n",
    "p.train()\n",
    "p.predict()\n",
    "\n",
    "p.classifier.plot_matrix_classification_report(TILE_CM, CP_PATH, PLOT_MATRIX_PATH, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Results :\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.85      0.86      0.86      5817\n",
    "           1       0.86      0.85      0.86      5880\n",
    "\n",
    "    accuracy                           0.86     11697\n",
    "   macro avg       0.86      0.86      0.86     11697\n",
    "weighted avg       0.86      0.86      0.86     11697\n",
    "```\n",
    "\n",
    "<img src='assets/data_analysis/data_clean_str_stop_words.plot.png' width='750'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Performances sur le dataset original en corrigeant les fautes d'ortographes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1/3\n",
    "ALPHA = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_MNB\n",
    "DATASET = 'dataset/csv/data_with_correction_spell.csv'\n",
    "\n",
    "PLOT_MATRIX_PATH = 'assets/data_analysis/data_with_correction_spell.plot.png'\n",
    "CP_PATH = 'assets/data_analysis/data_with_correction_spell_cp.txt'\n",
    "TILE_CM = 'data_with_correction_spell_CM'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, alpha=ALPHA, max_features=MAX_FEATURES)\n",
    "\n",
    "p.train()\n",
    "p.predict()\n",
    "\n",
    "p.classifier.plot_matrix_classification_report(TILE_CM, CP_PATH, PLOT_MATRIX_PATH, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Results :\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.83      0.85      0.84      6475\n",
    "         1.0       0.85      0.83      0.84      6651\n",
    "\n",
    "    accuracy                           0.84     13126\n",
    "   macro avg       0.84      0.84      0.84     13126\n",
    "weighted avg       0.84      0.84      0.84     13126\n",
    "```\n",
    "\n",
    "<img src='assets/data_analysis/data_with_correction_spell.plot.png' width='750'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Performances avec le dataset original réparti en 2 classes 0 et 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1/3\n",
    "ALPHA = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_MNB\n",
    "DATASET = 'dataset/csv/dataset_0-1.csv'\n",
    "\n",
    "PLOT_MATRIX_PATH = 'assets/data_analysis/dataset_0-1.plot.png'\n",
    "CP_PATH = 'assets/data_analysis/dataset_0-1_cp.txt'\n",
    "TILE_CM = 'dataset_0-1_CM'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, alpha=ALPHA, max_features=MAX_FEATURES)\n",
    "\n",
    "p.train()\n",
    "p.predict()\n",
    "\n",
    "p.classifier.plot_matrix_classification_report(TILE_CM, CP_PATH, PLOT_MATRIX_PATH, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Results :\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.85      0.86      0.86      5817\n",
    "           1       0.86      0.85      0.86      5880\n",
    "\n",
    "    accuracy                           0.86     11697\n",
    "   macro avg       0.86      0.86      0.86     11697\n",
    "weighted avg       0.86      0.86      0.86     11697\n",
    "```\n",
    "\n",
    "<img src='assets/data_analysis/dataset_0-1.plot.png' width='750'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Performances avec le dataset original réparti en 4 classes 0, 1, 2 et 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1/3\n",
    "ALPHA = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_MNB\n",
    "DATASET = 'dataset/csv/dataset_0-3.csv'\n",
    "\n",
    "PLOT_MATRIX_PATH = 'assets/data_analysis/dataset_0-3.plot.png'\n",
    "CP_PATH = 'assets/data_analysis/dataset_0-3_cp.txt'\n",
    "TILE_CM = 'dataset_0-3_CM'\n",
    "\n",
    "CLASSES = [0,1,2,3]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, alpha=ALPHA, max_features=MAX_FEATURES)\n",
    "\n",
    "p.train()\n",
    "p.predict()\n",
    "\n",
    "p.classifier.plot_matrix_classification_report(TILE_CM, CP_PATH, PLOT_MATRIX_PATH, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Results :\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.59      0.74      0.66      2672\n",
    "           1       0.48      0.07      0.13      1955\n",
    "           2       0.47      0.65      0.55      2644\n",
    "           3       0.74      0.74      0.74      2643\n",
    "\n",
    "    accuracy                           0.58      9914\n",
    "   macro avg       0.57      0.55      0.52      9914\n",
    "weighted avg       0.58      0.58      0.54      9914\n",
    "\n",
    "```\n",
    "\n",
    "<img src='assets/data_analysis/dataset_0-3.plot.png' width='750'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Différences entre les méthodes:\n",
    "> > ### MutlinomialNB en TFIDF\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Variation du test size (10% -> 50% par pas de 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_MNB\n",
    "\n",
    "DATASET = 'dataset/csv/dataset_0-1.csv'\n",
    "\n",
    "PLOT_ACC_PATH = 'assets/tfidf/mnb/mnb_test_size_dataset_0-1_acc.plot.png'\n",
    "PLOT_PREC_PATH = 'assets/tfidf/mnb/mnb_test_size_dataset_0-1_prec.plot.png'\n",
    "TITLE_PREC_ACC = 'mnb_test_size_dataset_0-1_Prec_Acc'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "        \n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "params = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "accuracies = []\n",
    "precisions = [[], []]\n",
    "\n",
    "for i in range(len(params)):\n",
    "\n",
    "    p = PipelineClassifier(CLASSIFIER, df, test_size=params[i], alpha=ALPHA, max_features=MAX_FEATURES)\n",
    "    p.train()\n",
    "    p.predict()\n",
    "    \n",
    "    accuracies.append(p.classifier.get_accuracy())\n",
    "    for c in CLASSES:\n",
    "        precisions[c].append(p.classifier.get_precisions(c))\n",
    "\n",
    "p.classifier.plot_accuracy_precisions(TITLE_PREC_ACC, PLOT_ACC_PATH, PLOT_PREC_PATH, TITLE_PREC_ACC, params, CLASSES, accuracies, precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Results\n",
    "> > ### Accuracy\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/mnb/mnb_test_size_dataset_0-1_acc.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> #### Precision\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/mnb/mnb_test_size_dataset_0-1_prec.plot.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Différences entre les méthodes:\n",
    "> > ### MutlinomialNB en TFIDF\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Variation de paramètre alpha (1 -> 6, par pas de 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_SIZE = 1/4\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_MNB\n",
    "DATASET = 'dataset/csv/dataset_0-1.csv'\n",
    "\n",
    "PLOT_ACC_PATH = 'assets/tfidf/mnb/mnb_alpha_dataset_0-1_acc.plot.png'\n",
    "PLOT_PREC_PATH = 'assets/tfidf/mnb/mnb_alpha_dataset_0-1_prec.plot.png'\n",
    "\n",
    "TITLE_PREC_ACC = 'mnb_alpha_dataset_0-1_Prec_Acc'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "params = [1,2,3,4,5,6]\n",
    "\n",
    "accuracies = []\n",
    "precisions = [[], []]\n",
    "\n",
    "for i in range(len(params)):\n",
    "\n",
    "    p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, alpha=params[i], max_features=MAX_FEATURES)\n",
    "    p.train()\n",
    "    p.predict()\n",
    "    \n",
    "    accuracies.append(p.classifier.get_accuracy())\n",
    "    for c in CLASSES:\n",
    "        precisions[c].append(p.classifier.get_precisions(c))\n",
    "\n",
    "p.classifier.plot_accuracy_precisions(TITLE_PREC_ACC, PLOT_ACC_PATH, PLOT_PREC_PATH, TITLE_PREC_ACC, params, CLASSES, accuracies, precisions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Results\n",
    "> > ### Accuracy\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/mnb/mnb_alpha_dataset_0-1_acc.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> #### Precision\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/mnb/mnb_alpha_dataset_0-1_prec.plot.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Différences entre les méthodes:\n",
    "> > ### Logistic Regression en TFIDF\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Variation de paramètre max_iter (250 -> 1000, par pas de 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1/4\n",
    "REG = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_LogReg\n",
    "DATASET = 'dataset/csv/dataset_0-1.csv'\n",
    "\n",
    "PLOT_ACC_PATH = 'assets/tfidf/log_reg/log_reg_max_iter_dataset_0-1_acc.plot.png'\n",
    "PLOT_PREC_PATH = 'assets/tfidf/log_reg/log_reg_max_iter_dataset_0-1_prec.plot.png'\n",
    "\n",
    "TITLE_PREC_ACC = 'log_reg_max_iter_dataset_0-1_Prec_Acc'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "params = [250, 500, 750, 1000]\n",
    "\n",
    "accuracies = []\n",
    "precisions = [[], []]\n",
    "\n",
    "for i in range(len(params)):\n",
    "\n",
    "    p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, max_iter=params[i], reg=REG, max_features=MAX_FEATURES)\n",
    "    p.train()\n",
    "    p.predict()\n",
    "    \n",
    "    accuracies.append(p.classifier.get_accuracy())\n",
    "    for c in CLASSES:\n",
    "        precisions[c].append(p.classifier.get_precisions(c))\n",
    "\n",
    "p.classifier.plot_accuracy_precisions(TITLE_PREC_ACC, PLOT_ACC_PATH, PLOT_PREC_PATH, TITLE_PREC_ACC, params, CLASSES, accuracies, precisions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Results\n",
    "> > ### Accuracy\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/log_reg/log_reg_max_iter_dataset_0-1_acc.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> #### Precision\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/log_reg/log_reg_max_iter_dataset_0-1_prec.plot.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Différences entre les méthodes:\n",
    "> > ### Logistic Regression en TFIDF\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Variation de paramètre de regularisation (0.25, 0.50, 0.75, 1.0, 12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 250\n",
    "TEST_SIZE = 1/4\n",
    "ALPHA = 1.0\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_LogReg\n",
    "DATASET = 'dataset/csv/dataset_0-1.csv'\n",
    "\n",
    "PLOT_ACC_PATH = 'assets/tfidf/log_reg/log_reg_reg_dataset_0-1_acc.plot.png'\n",
    "PLOT_PREC_PATH = 'assets/tfidf/log_reg/log_reg_reg_dataset_0-1_prec.plot.png'\n",
    "TITLE_PREC_ACC = 'log_reg_reg_dataset_0-1_Prec_Acc'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "params = [0.25, 0.5, 0.75, 1.0, 12.0]\n",
    "\n",
    "accuracies = []\n",
    "precisions = [[], []]\n",
    "\n",
    "for i in range(len(params)):\n",
    "\n",
    "    p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, max_iter=MAX_ITER, reg=params[i], max_features=MAX_FEATURES)\n",
    "    p.train()\n",
    "    p.predict()\n",
    "    \n",
    "    accuracies.append(p.classifier.get_accuracy())\n",
    "    for c in CLASSES:\n",
    "        precisions[c].append(p.classifier.get_precisions(c))\n",
    "\n",
    "p.classifier.plot_accuracy_precisions(TITLE_PREC_ACC, PLOT_ACC_PATH, PLOT_PREC_PATH, TITLE_PREC_ACC, params, CLASSES, accuracies, precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Results\n",
    "> > ### Accuracy\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/log_reg/log_reg_reg_dataset_0-1_acc.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> #### Precision\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/log_reg/log_reg_reg_dataset_0-1_prec.plot.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Différences entre les méthodes:\n",
    "> > ### Logistic Regression en TFIDF\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Variation de paramètre des max features (4500 -> 15000, par pas de 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 250\n",
    "TEST_SIZE = 1/4\n",
    "REG = 1.0\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_LogReg\n",
    "DATASET = 'dataset/csv/dataset_0-1.csv'\n",
    "\n",
    "PLOT_ACC_PATH = 'assets/tfidf/log_reg/log_reg_max_features_dataset_0-1_acc.plot.png'\n",
    "PLOT_PREC_PATH = 'assets/tfidf/log_reg/log_reg_max_features_dataset_0-1_prec.plot.png'\n",
    "TITLE_PREC_ACC = 'log_reg_max_features_dataset_0-1_Prec_Acc'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "params = [4500, 6000, 7500, 9000, 10500, 12000, 13500, 15000]\n",
    "\n",
    "accuracies = []\n",
    "precisions = [[], []]\n",
    "\n",
    "for i in range(len(params)):\n",
    "\n",
    "    p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, max_iter=MAX_ITER, reg=REG, max_features=params[i])\n",
    "    p.train()\n",
    "    p.predict()\n",
    "    \n",
    "    accuracies.append(p.classifier.get_accuracy())\n",
    "    for c in CLASSES:\n",
    "        precisions[c].append(p.classifier.get_precisions(c))\n",
    "\n",
    "p.classifier.plot_accuracy_precisions(TITLE_PREC_ACC, PLOT_ACC_PATH, PLOT_PREC_PATH, TITLE_PREC_ACC, params, CLASSES, accuracies, precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Results\n",
    "> > ### Accuracy\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/log_reg/log_reg_max_features_dataset_0-1_acc.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> #### Precision\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/log_reg/log_reg_max_features_dataset_0-1_prec.plot.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Différences entre les méthodes:\n",
    "> > ### Multi-layer perceptron en Word2Vec avec un modèle pré-entrainé\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Variation de paramètre des max iter (250 -> 1000, par pas de 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "LAYERS = (13, 13, 13)\n",
    "\n",
    "VEC_DIM = 200\n",
    "VEC_BIN = 'dataset/vectors/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin'\n",
    "\n",
    "CLASSIFIER = ClassifierType.WORD2VEC\n",
    "DATASET = 'dataset/csv/dataset_0-1.csv'\n",
    "\n",
    "PLOT_ACC_PATH = 'assets/w2v/mlp/word2vec_max_iter_acc_0-1.plot.png'\n",
    "PLOT_PREC_PATH = 'assets/w2v/mlp/word2vec_max_iter_prec_0-1.plot.png'\n",
    "\n",
    "TITLE_PREC_ACC = 'max_iteration'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "params = [250, 500, 750, 1000]\n",
    "\n",
    "accuracies = []\n",
    "precisions = [[], []]\n",
    "\n",
    "for i in range(len(params)):\n",
    "\n",
    "    p = PipelineClassifier(CLASSIFIER, data=df, vec_bin=VEC_BIN, max_iter=params[i], layers=LAYERS, vec_dim=VEC_DIM, test_size=TEST_SIZE)\n",
    "    p.train()\n",
    "    p.predict()\n",
    "    \n",
    "    accuracies.append(p.classifier.get_accuracy())\n",
    "    for c in CLASSES:\n",
    "        precisions[c].append(p.classifier.get_precisions(c))\n",
    "\n",
    "p.classifier.plot_accuracy_precisions(TITLE_PREC_ACC, PLOT_ACC_PATH, PLOT_PREC_PATH, TITLE_PREC_ACC, params, CLASSES, accuracies, precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Results\n",
    "> > ### Accuracy\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/w2v/mlp/word2vec_max_iter_acc_0-1.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> #### Precision\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/w2v/mlp/word2vec_max_iter_prec_0-1.plot.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Différences entre les méthodes:\n",
    "> > ### Multi-layer perceptron en Word2Vec avec un modèle pré-entrainé\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Variation de paramètre de la taille des layers (1 -> 9 , par pas de 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 500\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "VEC_DIM = 200\n",
    "VEC_BIN = 'dataset/vectors/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin'\n",
    "\n",
    "CLASSIFIER = ClassifierType.WORD2VEC\n",
    "DATASET = 'dataset/csv/dataset_0-1.csv'\n",
    "\n",
    "PLOT_ACC_PATH = 'assets/w2v/mlp/word2vec_layers_acc_0-1.plot.png'\n",
    "PLOT_PREC_PATH = 'assets/w2v/mlp/word2vec_layers_prec_0-1.plot.png'\n",
    "TITLE_PREC_ACC = 'hidden layers'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "params = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "accuracies = []\n",
    "precisions = [[], []]\n",
    "\n",
    "p = PipelineClassifier(CLASSIFIER, data=df, vec_bin=VEC_BIN, max_iter=MAX_ITER, layers=None, vec_dim=VEC_DIM, test_size=TEST_SIZE)\n",
    "p.transform_data()\n",
    "\n",
    "for i in range(len(params)):\n",
    "    \n",
    "    p.classifier.classifier = ClassifierWord2Vec(\n",
    "        data=df,\n",
    "        word2vec_bin=VEC_BIN,\n",
    "        max_iter=MAX_ITER,\n",
    "        layers= [13] * params[i],\n",
    "        vec_dim=VEC_DIM,\n",
    "        test_size=TEST_SIZE\n",
    "    )\n",
    "    p.train_without_transform()\n",
    "    p.predict()\n",
    "    \n",
    "    accuracies.append(p.classifier.get_accuracy())\n",
    "    for c in CLASSES:\n",
    "        precisions[c].append(p.classifier.get_precisions(c))\n",
    "\n",
    "p.classifier.plot_accuracy_precisions(TITLE_PREC_ACC, PLOT_ACC_PATH, PLOT_PREC_PATH, TITLE_PREC_ACC, params, CLASSES, accuracies, precisions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Results\n",
    "> > ### Accuracy\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/w2v/mlp/word2vec_layers_acc_0-1.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> #### Precision\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/w2v/mlp/word2vec_layers_prec_0-1.plot.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Différences entre les méthodes:\n",
    "> > ### Naives Bayes avec un dictionnaires de features\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Variation de paramètre du nombre d'entrées du dictionnaire (1500 -> 6000 , par pas de 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "\n",
    "CLASSIFIER = ClassifierType.NAIVES_BAYES\n",
    "DATASET = 'dataset/csv/dataset_0-1.csv'\n",
    "\n",
    "PLOT_ACC_PATH = 'assets/features/nb/naives_bayes_nb_word_acc_0-1.plot.png'\n",
    "PLOT_PREC_PATH = 'assets/features/nb/naives_bayes_nb_word_prec_0-1.plot.png'\n",
    "TITLE_PREC_ACC = 'number of words'\n",
    "\n",
    "CLASSES = [0,1]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "params = [1500, 3000, 4500, 6000]\n",
    "\n",
    "accuracies = []\n",
    "precisions = [[], []]\n",
    "\n",
    "for i in range(len(params)):\n",
    "\n",
    "    p = PipelineClassifier(CLASSIFIER, data=df, nb_word_n=params[i], test_size=TEST_SIZE)\n",
    "    p.train()\n",
    "    p.predict()\n",
    "    \n",
    "    accuracies.append(p.classifier.get_accuracy())\n",
    "    for c in CLASSES:\n",
    "        precisions[c].append(p.classifier.get_precisions(c))\n",
    "\n",
    "p.classifier.plot_accuracy_precisions(TITLE_PREC_ACC, PLOT_ACC_PATH, PLOT_PREC_PATH, TITLE_PREC_ACC, params, CLASSES, accuracies, precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Results\n",
    "> > ### Accuracy\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/features/nb/naives_bayes_nb_word_acc_0-1.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> #### Precision\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/features/nb/naives_bayes_nb_word_prec_0-1.plot.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Cross Validation \n",
    "> > Nous avons utilisé la cross-validation afin de trouver la meilleur combinaison de paramètres pour chaque modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Cross Validation des modèles TFIDF en Logistic Regression, MultinomialNB, et Mutli-layer percreptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def grid(classifier, params, X_train, X_test, y_train, y_test, classes, matrix_path, cp_path, title):\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "    grid_search = GridSearchCV(classifier, cv=5, param_grid=params)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(f\"classifier : {classifier}\")\n",
    "    print(f\"parameters : {params}\")\n",
    "    \n",
    "    initial_t = time()\n",
    "    # on fit les datas à notre grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - initial_t))\n",
    "    print()\n",
    "    print(\"---------------SCORE-------------\")\n",
    "    print(grid_search.cv_results_)\n",
    "\n",
    "    # On récupère le meilleur score de prédiction ( à priori équivalent à la précision)\n",
    "\n",
    "    print(\"Best CV score : %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set: \")\n",
    "    # On donne également les meilleurs paramètres\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()) :\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    print(\"Test score with best_estimator_ : %0.3f\" % grid_search.best_estimator_.score(X_test, y_test))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "\n",
    "    y_test = y_test\n",
    "    predic = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    confm = confusion_matrix(y_test, predic)\n",
    "    df_cm = pd.DataFrame(confm, index=classes, columns=classes)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    ax.set_title('Confusion matrix for '+ title)\n",
    "    sb.heatmap(df_cm, cmap='YlOrRd', annot=True, fmt='g', ax=ax)\n",
    "    plt.savefig(matrix_path)\n",
    "\n",
    "    c_r = classification_report(y_test, predic)\n",
    "    f = open(cp_path, 'a')\n",
    "    f.write(c_r)\n",
    "    f.close()\n",
    "\n",
    "def replace_nan(df):\n",
    "    for i in range(df.shape[0]):\n",
    "        if pd.isna(df[\"avis\"][i]):\n",
    "            if df[\"classe_bon_mauvais\"][i] == 1: \n",
    "                df.at[i, \"avis\"] = \"good\"\n",
    "            else:\n",
    "                df.at[i, \"avis\"] = \"bad\"\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./dataset/csv/dataset_0-1.csv\")\n",
    "\n",
    "df = replace_nan(df)\n",
    "\n",
    "X = df['avis'].copy()  # X correspond aux reviews\n",
    "y = df['classe_bon_mauvais'].copy()  # y correspond aux classes comme c'est ce que l'on cherche à prévoir \n",
    "\n",
    "# Extractions des features\n",
    "td = TfidfVectorizer(max_features=9000)\n",
    "X = td.fit_transform(X).toarray()\n",
    "\n",
    "# On split les datas en différents ensemble d'entrainement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/4, random_state=0)\n",
    "\n",
    "logreg_classifier = LogisticRegression(C=1, max_iter=250).fit(X_train, y_train) # max_iter à 100 de base, faut monter ici car trop de data\n",
    "logreg_score = logreg_classifier.score(X_test, y_test)\n",
    "\n",
    "mnb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "mnb_score = mnb_classifier.score(X_test, y_test)\n",
    "\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=[13,13,13], max_iter=500).fit(X_train, y_train)\n",
    "mlp_score = mlp_classifier.score(X_test, y_test)\n",
    "\n",
    "print('======================================================')\n",
    "# print(f\"\\n LogReg score {logreg_score}\")\n",
    "# print(f\"\\n MNB score {mnb_score}\")\n",
    "print(f\"\\n MLPC score {mlp_score}\")\n",
    "print('======================================================')\n",
    "\n",
    "param_logreg_grid_ = {\n",
    "    'C': [0.75, 1.0],\n",
    "    'max_iter' : [250, 500]\n",
    "    }\n",
    "# as for the alpha param mnb, c is the hyperparameter of log reg\n",
    "param_mnb_grid_ ={'alpha': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]}\n",
    "\n",
    "param_mlp_grid_ = {\n",
    "    'hidden_layer_sizes' : [[13,13,13,13], [13,13,13,13,13]],\n",
    "    'max_iter' : [750, 1000]\n",
    "}\n",
    "\n",
    "classes = [0,1]\n",
    "PLOT_MATRIX_PATH_LR = 'assets/tfidf/grid_search/log_reg/grid_search_logreg_dataset_0-1.plot.png'\n",
    "CP_PATH_LR = 'assets/tfidf/grid_search/log_reg/grid_search_logreg_dataset_0-1_cp.txt'\n",
    "TILE_CM_LR = 'grid_search_logreg_dataset_0-1_CM'\n",
    "\n",
    "PLOT_MATRIX_PATH_MNB = 'assets/tfidf/grid_search/mnb/grid_search_mnb_dataset_0-1.plot.png'\n",
    "CP_PATH_MNB = 'assets/tfidf/grid_search/mnb/grid_search_mnb_dataset_0-1_cp.txt'\n",
    "TILE_CM_MNB = 'grid_search_mnb_dataset_0-1_CM'\n",
    "\n",
    "PLOT_MATRIX_PATH_MLP = 'assets/tfidf/grid_search/mlp/grid_search_mlp_dataset_0-1.plot.png'\n",
    "CP_PATH_MLP = 'assets/tfidf/grid_search/mlp/grid_search_mlp_dataset_0-1_cp.txt'\n",
    "TILE_CM_MLP = 'grid_search_mlp_dataset_0-1_CM'\n",
    "\n",
    "# to understand alpha param https://stackoverflow.com/questions/33830959/multinomial-naive-bayes-parameter-alpha-setting-scikit-learn\n",
    "grid(mnb_classifier, param_mnb_grid_, X_train, X_test, y_train, y_test, classes, PLOT_MATRIX_PATH_MNB, CP_PATH_MNB, TILE_CM_MNB)\n",
    "\n",
    "grid(logreg_classifier, param_logreg_grid_, X_train, X_test, y_train, y_test, classes, PLOT_MATRIX_PATH_LR, CP_PATH_LR, TILE_CM_LR)\n",
    "\n",
    "grid(mlp_classifier, param_mlp_grid_, X_train, X_test, y_train, y_test, classes, PLOT_MATRIX_PATH_MLP, CP_PATH_MLP, TILE_CM_MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Conclusion\n",
    "> > Meilleurs modèles de TFIDF: \n",
    "\n",
    "> ## Logistic Regression\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/grid_search/log_reg/grid_search_logreg_dataset_0-1.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.85      0.87      0.86      4325\n",
    "           1       0.87      0.85      0.86      4460\n",
    "\n",
    "    accuracy                           0.86      8785\n",
    "   macro avg       0.86      0.86      0.86      8785\n",
    "weighted avg       0.86      0.86      0.86      8785\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "> ## Multinomial Naives Bayes\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/grid_search/mnb/grid_search_mnb_dataset_0-1.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.85      0.85      4325\n",
    "           1       0.86      0.85      0.85      4460\n",
    "\n",
    "    accuracy                           0.85      8785\n",
    "   macro avg       0.85      0.85      0.85      8785\n",
    "weighted avg       0.85      0.85      0.85      8785\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "> ## Multinomial Multi-layer perceptron\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='assets/tfidf/grid_search/mlp/grid_search_mlp_dataset_0-1.plot.png' width='500'>\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.82      0.83      4361\n",
    "           1       0.82      0.83      0.83      4412\n",
    "\n",
    "    accuracy                           0.83      8773\n",
    "   macro avg       0.83      0.83      0.83      8773\n",
    "weighted avg       0.83      0.83      0.83      8773\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Meilleurs score : TFIDF Logistique Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 250\n",
    "TEST_SIZE = 1/4\n",
    "REG = 1.0\n",
    "MAX_FEATURES = 9000\n",
    "\n",
    "CLASSIFIER = ClassifierType.TFIDF_LogReg\n",
    "DATASET = 'dataset/csv/dataset_0-3.csv'\n",
    "\n",
    "PLOT_MATRIX_PATH = 'assets/tfidf/log_reg/log_reg_best_params_dataset_0-3.plot.png'\n",
    "CP_PATH = 'assets/tfidf/log_reg/log_reg_best_params_dataset_0-3_cp.txt'\n",
    "TILE_CM = 'log_reg_best_params_dataset_0-3_CM'\n",
    "\n",
    "CLASSES = [0,1,2,3]\n",
    "\n",
    "df = pd.read_csv(DATASET)[['classe_bon_mauvais', 'avis']]\n",
    "\n",
    "p = PipelineClassifier(CLASSIFIER, df, test_size=TEST_SIZE, max_iter=MAX_ITER, reg=REG, max_features=MAX_FEATURES)\n",
    "\n",
    "p.train()\n",
    "p.predict()\n",
    "\n",
    "p.classifier.plot_matrix_classification_report(TILE_CM, CP_PATH, PLOT_MATRIX_PATH, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Results :\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.60      0.71      0.65      1998\n",
    "           1       0.40      0.27      0.33      1465\n",
    "           2       0.51      0.55      0.53      1959\n",
    "           3       0.76      0.74      0.75      2014\n",
    "\n",
    "    accuracy                           0.59      7436\n",
    "   macro avg       0.57      0.57      0.57      7436\n",
    "weighted avg       0.58      0.59      0.58      7436\n",
    "\n",
    "```\n",
    "\n",
    "<img src='assets/tfidf/log_reg/log_reg_best_params_dataset_0-3.plot.png' width='750'>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c129b8a8528476873ea75e8693e58dad88b8b6f73143c4ab65da825016900d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
