{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Traitement Automatique de texte en IA projet\n",
    "> \n",
    "> > Antoine Vidal-Mazuy - Yann Brault\n",
    "> > 28/12/2021 Université Côtes d'azur\n",
    "\n",
    "<br>\n",
    "\n",
    "> ## Introduction\n",
    "\n",
    "<br>\n",
    "\n",
    "> blablzqdbd bzqjkdb qz\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> ## Datasets utilisés\n",
    ">\n",
    "> > #### Datasets de base:\n",
    "\n",
    "<br>\n",
    "\n",
    "> Nous avons décidé de construire nous même notre base de donnée. Pour cela nous avons conçu un petit scraper su site Jeux-Video.com. <br>\n",
    "> Le code source du scraper se trouve dans le lien suivant [JVCScraper](https://github.com/Brotherta/JVCScraper).\n",
    "\n",
    "<br>\n",
    "\n",
    "> Nous avons récupéré tous les commentaires et notes d'utilisateurs sur environ 50 jeux, pour un total de 102000 avis. Puis nous les avons sauvegarder sous forme de fichiers csv. <br>\n",
    "> Nous avons dû nettoyer les avis de tous les charactères spéciaux, des mots de liaisons, les urls et tout ce qui n'apportait rien à la compréhension de l'avis.\n",
    "> Afin de nettoyer plus facilement les commentaires nous avons créer une classe **CleanData** qui nous permettait d'effectuer plusieurs tâches de pré-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "\n",
    "class CleanData:\n",
    "\n",
    "    def __init__(self, max_words, df: DataFrame = None) -> None:\n",
    "        self.max_words = max_words\n",
    "        self.df = df\n",
    "        self.unused_chars = ',|;|\\&|\\#|\\@|\\%|\\:|\\>|\\<|\\(|\\)|\\{|\\}|\\=|\\+|\\_|\\[|\\}|\\^|\\*|\\!|\\?|\\/|\\¨|\\~|\\\\\\|\\§|\\||[0-9]|\\[|\\]|\\\"'\n",
    "        self.connecting_words = [\n",
    "            \"c'est\", \"ces\", \"ses\", \"s'est\", \"a\", \"de\", \"du\", \n",
    "            \"et\", \"le\", \"les\", \"un\", \"une\", \"pour\", \"sur\", \"etc\", \"est\", \"c\",\n",
    "            'la', \"jeu\", \"que\", \"des\", \"en\", \"ce\", \"qu\", \"ca\", \"y\", \"je\", \"sa\", \"son\",\n",
    "            \"au\", \"ai\", \"mon\", \"ma\", \"mes\", \"qui\", \"je\", \"tu\", \"il\", \"ils\", \"elles\", \"elle\", \"vous\", \"nous\",\n",
    "            \"qu'il\", \"qu'elle\", \"qu'ils\", \"qu'elles\", \"qu'on\",\n",
    "            \"on\", \"se\", \"par\"]\n",
    "        self.urls = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b'\n",
    "            \n",
    "        self.spell = SpellChecker(language='fr')\n",
    "\n",
    "    def correction_spelling(self, review):\n",
    "        \"\"\" \n",
    "        try to elimiminates the unknown words of a sentence, and \n",
    "        replacing it by a correct word. \n",
    "        \"\"\"\n",
    "        \n",
    "        review_list = review.split(\" \")\n",
    "        \n",
    "        bad = []\n",
    "        for word in review_list:\n",
    "            if word != \" \":\n",
    "                bad = self.spell.unknown(review_list)\n",
    "\n",
    "        new_list = []\n",
    "        for word in review_list:\n",
    "            if word in bad:\n",
    "                new_list.append(self.spell.correction(word))\n",
    "            else:\n",
    "                new_list.append(word)\n",
    "        \n",
    "        return ' '.join(new_list)\n",
    "\n",
    "    def replace_nan(self):\n",
    "        \"\"\"\n",
    "        Replace nan by 'bon' or 'mauvais' in the dataframe.\n",
    "        \"\"\"\n",
    "        to_drop = []\n",
    "        for i in self.df.index:\n",
    "            r = self.df['avis'][i]\n",
    "            if pd.isna(r) or r in ['nan', 'Nan'] or type(r) == float:\n",
    "                to_drop.append(i)\n",
    "        print(\"droped nan : \", len(to_drop))\n",
    "        self.df = self.df.drop(to_drop)\n",
    "\n",
    "    def remove_urls(self, review):\n",
    "        review = re.sub(self.urls, '', review, flags=re.MULTILINE)\n",
    "        return(review)\n",
    "\n",
    "    def clean_str(self, review):\n",
    "        \"\"\"\n",
    "        Remove special characters from the string.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(review) > 0 or review != None:\n",
    "            review = re.sub(self.unused_chars, ' ', review)\n",
    "            review = review.replace('.', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('\\n', ' ')\n",
    "            review = review.lower()\n",
    "            review = re.sub(' +', ' ', review)\n",
    "            review = re.sub(r' (?! ) ', '', review) #removing single characters\n",
    "       \n",
    "        return review\n",
    "    \n",
    "    def clean_stop_words(self, review):\n",
    "        review_list = review.split(\" \")\n",
    "\n",
    "        new_list = []\n",
    "        for word in review_list:\n",
    "            if word != \" \" and not word in self.connecting_words and len(word) > 1:\n",
    "                new_list.append(word)\n",
    "\n",
    "        return ' '.join(new_list)\n",
    "\n",
    "    def clean_review(self, review):\n",
    "        review = self.clean_str(review)\n",
    "        review = self.clean_stop_words(review)\n",
    "        review = self.correction_spelling(review)\n",
    "        \n",
    "        return review\n",
    "\n",
    "    def clean_dataset(self):\n",
    "        \"\"\"\n",
    "        Main method call to prepare a text to be vectorized.\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = self.replace_nan()\n",
    "        for i in trange(self.df.shape[0]):\n",
    "            review = self.df.at[i, 'avis']\n",
    "            review = self.clean_str(review)\n",
    "            review = self.clean_stop_words(review)\n",
    "            review = self.correction_spelling(review)\n",
    "                \n",
    "            self.df.at[i, 'avis'] = review\n",
    "\n",
    "    def filter_long_review(self):\n",
    "        \"\"\"\n",
    "        Filter the string with too many words.\n",
    "        \"\"\"\n",
    "\n",
    "        to_drop = []\n",
    "        for i in self.df.index:\n",
    "            review = self.df['avis'][i]\n",
    "            review_list = review.split()\n",
    "\n",
    "            if len(review_list) > self.max_words:\n",
    "                to_drop.append(i)\n",
    "\n",
    "        self.df = self.df.drop(to_drop)\n",
    "        print(f\"dropped {len(to_drop)} lines\")\n",
    "\n",
    "    def fix_repartition_for_4_classes(self):\n",
    "        \"\"\"\n",
    "        Fix the bad repartitions of the dataset, by removing randomly good reviews.\n",
    "        \"\"\"\n",
    "\n",
    "        d = self.df.groupby(['classe_bon_mauvais'], as_index=False).count()\n",
    "        nb_bad = d['avis'][0]\n",
    "\n",
    "        nb_good = d['avis'][2]\n",
    "        to_remove = nb_good - nb_bad\n",
    "\n",
    "        while(to_remove > 0):\n",
    "            row: DataFrame = self.df.sample()\n",
    "            index = row.first_valid_index()\n",
    "            print(f\"{to_remove}\")\n",
    "\n",
    "            if row['classe_bon_mauvais'][index] == 2:\n",
    "                self.df.drop(index, inplace=True)\n",
    "                to_remove -= 1\n",
    "\n",
    "\n",
    "        nb_good = d['avis'][3]\n",
    "        to_remove = nb_good - nb_bad\n",
    "\n",
    "        while(to_remove > 0):\n",
    "            row: DataFrame = self.df.sample()\n",
    "            index = row.first_valid_index()\n",
    "            print(f\"{to_remove}\")\n",
    "\n",
    "            if row['classe_bon_mauvais'][index] == 3:\n",
    "                self.df.drop(index, inplace=True)\n",
    "                to_remove -= 1\n",
    "        \n",
    "        d = self.df.groupby(['classe_bon_mauvais'], as_index=False).count()\n",
    "        nb_bad = d['avis'][0]\n",
    "        nb_good = d['avis'][1]\n",
    "        print(\"2\", nb_bad, nb_good)\n",
    "\n",
    "    def fix_repartition(self):\n",
    "        \"\"\"\n",
    "        Fix the bad repartitions of the dataset, by removing randomly good advice.\n",
    "        \"\"\"\n",
    "\n",
    "        d = self.df.groupby(['classe_bon_mauvais'], as_index=False).count()\n",
    "        nb_bad = d['avis'][0]\n",
    "        nb_good = d['avis'][1]\n",
    "        print(nb_bad, nb_good)\n",
    "\n",
    "        to_remove = nb_good - nb_bad\n",
    "        \n",
    "        while(to_remove > 0):\n",
    "            row: DataFrame = self.df.sample()\n",
    "            index = row.first_valid_index()\n",
    "            print(f\"{to_remove}\")\n",
    "\n",
    "            if row['classe_bon_mauvais'][index] == 1:\n",
    "                self.df.drop(index, inplace=True)\n",
    "                to_remove -= 1\n",
    "        \n",
    "        d = self.df.groupby(['classe_bon_mauvais'], as_index=False).count()\n",
    "        nb_bad = d['avis'][0]\n",
    "        nb_good = d['avis'][1]\n",
    "        print(\"2\", nb_bad, nb_good)\n",
    "\n",
    "    def save_data(self, path):\n",
    "        self.df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Pre-processing\n",
    "> Données avant pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aller, un bon 10 parce que ca reste jouable, mais systeme de paiement si tu veut plus, un jeu copier coller de l'ancienne version ,et on recommence ! pas besoin d'en dire plus\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/csv/dataset_original.csv')\n",
    "\n",
    "df['avis'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Données après pre-processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"aller bon parce reste jouable mais systeme paiement si veut plus copier coller l'ancienne version recommence pas besoin d'en dire plus\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/csv/dataset_0-1.csv')\n",
    "\n",
    "df['avis'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nous avons calculer les 30 mots les plus utilisés que l'on a par la suite enlevé. parmis eux :\n",
    "\n",
    "```\n",
    "\"c'est\", \"ces\", \"ses\", \"s'est\", \"a\", \"de\", \"du\", \"et\", \"le\", \"les\", \"un\", \"une\", \"pour\", \"sur\", \"etc\", \"est\", \"c\",\n",
    "'la', \"jeu\", \"que\", \"des\", \"en\", \"ce\", \"qu\", \"ca\", \"y\", \"je\", \"sa\", \"son\",\"au\", \"ai\", \"mon\", \"ma\", \"mes\", \"qui\", \"je\", \n",
    "\"tu\", \"il\", \"ils\", \"elles\", \"elle\", \"vous\", \"nous\",\"qu'il\", \"qu'elle\", \"qu'ils\", \"qu'elles\", \"qu'on\", \"on\", \"se\", \"par\"\n",
    "```\n",
    "\n",
    "> Les avis trop longs posent aussi problèmes. Certains avis comportaient plus de 1000 mots, pour une moyenne beaucoup plus basse. <br>\n",
    "> Cela pose problème pour l'entrainement mais aussi pour la représentation des avis.\n",
    "\n",
    "<br>\n",
    "\n",
    "> ## Classes des notes\n",
    "> > Pour représenter les notes, nous avons instaurer 2 classes différentes, 0 pour les notes < 12, et 1 pour les notes >= 12.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Dataset 2 classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classe_bon_mauvais</th>\n",
       "      <th>avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23587</th>\n",
       "      <td>0</td>\n",
       "      <td>navet siècle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19275</th>\n",
       "      <td>0</td>\n",
       "      <td>selon moi aucun ne mérite donc cela lui donne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19606</th>\n",
       "      <td>1</td>\n",
       "      <td>ac iv black flag réussite trouve l'ambiance at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8150</th>\n",
       "      <td>0</td>\n",
       "      <td>qu'est-ce si spécial durée vie mode solo plus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>1</td>\n",
       "      <td>graphismes vieillissent pas trop mal histoire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12249</th>\n",
       "      <td>0</td>\n",
       "      <td>suis généreux pc n'y pas différence mais j'ai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17979</th>\n",
       "      <td>1</td>\n",
       "      <td>très bien mais comprends l'intérêt foutre mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>0</td>\n",
       "      <td>nul car toujours dans couloir l'on perd tout t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35057</th>\n",
       "      <td>0</td>\n",
       "      <td>bon donne moyenne parce ff mais honnêtement lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11414</th>\n",
       "      <td>0</td>\n",
       "      <td>déroulement trop linéaire manque possibilité d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35138 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       classe_bon_mauvais                                               avis\n",
       "23587                   0                                       navet siècle\n",
       "19275                   0      selon moi aucun ne mérite donc cela lui donne\n",
       "19606                   1  ac iv black flag réussite trouve l'ambiance at...\n",
       "8150                    0  qu'est-ce si spécial durée vie mode solo plus ...\n",
       "3308                    1  graphismes vieillissent pas trop mal histoire ...\n",
       "...                   ...                                                ...\n",
       "12249                   0  suis généreux pc n'y pas différence mais j'ai ...\n",
       "17979                   1  très bien mais comprends l'intérêt foutre mont...\n",
       "1745                    0  nul car toujours dans couloir l'on perd tout t...\n",
       "35057                   0  bon donne moyenne parce ff mais honnêtement lo...\n",
       "11414                   0  déroulement trop linéaire manque possibilité d...\n",
       "\n",
       "[35138 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/csv/dataset_0-1.csv')\n",
    "\n",
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset 4 classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classe_bon_mauvais</th>\n",
       "      <th>avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8386</th>\n",
       "      <td>2</td>\n",
       "      <td>ah call of duty mais où donc passé cette série...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26616</th>\n",
       "      <td>0</td>\n",
       "      <td>ah faut arrêter avec maintenant serait peut-êt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24699</th>\n",
       "      <td>0</td>\n",
       "      <td>un zelda insipide on prend fait depuis décenni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5359</th>\n",
       "      <td>3</td>\n",
       "      <td>ce vraiment simulation combat top top concerne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19623</th>\n",
       "      <td>3</td>\n",
       "      <td>assez bon avec beaux graphismes addiction plut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26252</th>\n",
       "      <td>2</td>\n",
       "      <td>un bon jeux promet belle chose attendre pour l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25813</th>\n",
       "      <td>3</td>\n",
       "      <td>ah skyrim un j'ai tant attendu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>3</td>\n",
       "      <td>un très bon halo le problème one manque puissa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19294</th>\n",
       "      <td>3</td>\n",
       "      <td>j'ai découvert jv com n'ai vraiment jamais prê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1</td>\n",
       "      <td>pour part j'ai joué premières générations en r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       classe_bon_mauvais                                               avis\n",
       "8386                    2  ah call of duty mais où donc passé cette série...\n",
       "26616                   0  ah faut arrêter avec maintenant serait peut-êt...\n",
       "24699                   0  un zelda insipide on prend fait depuis décenni...\n",
       "5359                    3  ce vraiment simulation combat top top concerne...\n",
       "19623                   3  assez bon avec beaux graphismes addiction plut...\n",
       "...                   ...                                                ...\n",
       "26252                   2  un bon jeux promet belle chose attendre pour l...\n",
       "25813                   3                    ah skyrim un j'ai tant attendu \n",
       "2003                    3  un très bon halo le problème one manque puissa...\n",
       "19294                   3  j'ai découvert jv com n'ai vraiment jamais prê...\n",
       "376                     1  pour part j'ai joué premières générations en r...\n",
       "\n",
       "[29748 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/csv/dataset_0-3.csv')\n",
    "\n",
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Corretions des mots\n",
    "> > Afin de tenir comptes des mots mal orthographiés, nous avons utilisés la librairie [spellchecker](https://pypi.org/project/pyspellchecker/). <br>\n",
    "> > Malheureusement nous avons fait une erreur dans le code. En effet nous avons transformé tous les points et toutes les virgules par du vide. <br>\n",
    "> > Nous avons perdu par ce biais beaucoup de mots car inutilisables pour la méthode Word2Vec.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c129b8a8528476873ea75e8693e58dad88b8b6f73143c4ab65da825016900d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
